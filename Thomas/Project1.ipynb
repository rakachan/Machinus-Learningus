{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Define seed for train/test random splitting\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "DATA_TEST_PATH = 'data/test.csv'\n",
    "OUTPUT_PATH = 'data/output_ridge_regression.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "#_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from crossvalidation import *\n",
    "\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1, data2, data3, y1, y2, y3 = split_models_and_process(tX, y)\n",
    "#test1, test2, test3, id1, id2, id3 = split_models_and_process(tX_test, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 19)\n",
      "(77544, 22)\n",
      "(72543, 29)\n",
      "[[ 1.          1.32665921  1.31515584 ..., -0.64777054  1.43938739\n",
      "  -0.64922666]\n",
      " [ 1.         -0.23872136  2.6223173  ..., -0.67036471  0.64019437\n",
      "  -0.64922666]\n",
      " [ 1.          0.57777524  1.80164648 ..., -0.64864421  2.50094947\n",
      "  -0.64922666]\n",
      " ..., \n",
      " [ 1.          0.76270284  1.00306018 ..., -0.67026763  1.30217208\n",
      "  -0.64922666]\n",
      " [ 1.         -0.17933582  1.02075207 ..., -0.62954473  2.08629422\n",
      "  -0.64922666]\n",
      " [ 1.          1.11646788  1.06975061 ..., -0.68795953  1.76320483\n",
      "  -0.64922666]]\n"
     ]
    }
   ],
   "source": [
    "print(data1.shape)\n",
    "print(data2.shape)\n",
    "print(data3.shape)\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_w = np.zeros(data1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08262135 -0.40683861 -0.0454093   0.00602996  0.05713162  0.00602995\n",
      "  0.15486163  0.04365706  0.06135347  0.22866656  0.0538303   0.05255455\n",
      " -0.02016495  0.05467388  0.05417643 -0.07639998  0.05251972  0.0309534\n",
      "  0.05363998]\n"
     ]
    }
   ],
   "source": [
    "weights, loss = least_squares_GD(y1, data1, initial_w, 500, 0.01)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09821149 -0.35305428  0.04573601 -0.02224016  0.06606689 -0.02224016\n",
      "  0.26275693  0.05087699  0.07955536  0.30592512  0.07320706  0.05198523\n",
      "  0.02059563  0.07247051  0.07339587 -0.07289699  0.06967975  0.00293434\n",
      "  0.06376152]\n"
     ]
    }
   ],
   "source": [
    "weights, loss = least_squares_SGD(y1, data1, initial_w, 500, 0.01)\n",
    "# Also overflows\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.62325772e+00  -3.43542541e-01  -4.47575878e-01   1.17974737e+03\n",
      "   2.14094541e+01  -1.17986774e+03  -9.86222474e+00  -1.23927230e+01\n",
      "   1.43692920e+00   1.03930964e+01  -3.56679923e-02  -1.45849741e-02\n",
      "   1.05088974e+01   2.09445736e-01   1.36247577e-03   2.12008942e-01\n",
      "  -1.20433766e-01   2.59863682e-02  -5.50059896e+00]\n"
     ]
    }
   ],
   "source": [
    "weights, loss = least_squares(y1, data1)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for a jet = 0\n",
      "0 - Training accuracy: 0.802967 / Test accuracy : 0.805525\n",
      "1 - Training accuracy: 0.802923 / Test accuracy : 0.806926\n",
      "2 - Training accuracy: 0.803601 / Test accuracy : 0.806426\n",
      "3 - Training accuracy: 0.803423 / Test accuracy : 0.803023\n",
      "4 - Training accuracy: 0.804202 / Test accuracy : 0.798118\n",
      "5 - Training accuracy: 0.803023 / Test accuracy : 0.806126\n",
      "6 - Training accuracy: 0.804402 / Test accuracy : 0.793314\n",
      "7 - Training accuracy: 0.804924 / Test accuracy : 0.813732\n",
      "8 - Training accuracy: 0.803601 / Test accuracy : 0.798819\n",
      "9 - Training accuracy: 0.803178 / Test accuracy : 0.803323\n",
      "\n",
      "Average test accuracy: 0.803533\n",
      "Variance test accuracy: 0.000029\n",
      "Min test accuracy: 0.793314\n",
      "Max test accuracy: 0.813732\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "\n",
    "# Model parameters\n",
    "lamb = 0.002\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "max_acc = 0\n",
    "w1 = []\n",
    "\n",
    "print(\"Accuracy for a jet = 0\")\n",
    "k_indices = build_k_indices(y1, k_fold, seed)\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test, weights = cross_validation(y1, data1, k_indices, k, ridge_regression, lambda_=lamb)\n",
    "    if (acc_test>max_acc):\n",
    "        w1 = weights\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for a jet = 1\n",
      "0 - Training accuracy: 0.698063 / Test accuracy : 0.704153\n",
      "1 - Training accuracy: 0.699481 / Test accuracy : 0.690740\n",
      "2 - Training accuracy: 0.699252 / Test accuracy : 0.694609\n",
      "3 - Training accuracy: 0.699109 / Test accuracy : 0.700155\n",
      "4 - Training accuracy: 0.698908 / Test accuracy : 0.703895\n",
      "5 - Training accuracy: 0.698951 / Test accuracy : 0.697575\n",
      "6 - Training accuracy: 0.698750 / Test accuracy : 0.699768\n",
      "7 - Training accuracy: 0.698192 / Test accuracy : 0.705571\n",
      "8 - Training accuracy: 0.699625 / Test accuracy : 0.691901\n",
      "9 - Training accuracy: 0.699080 / Test accuracy : 0.699897\n",
      "\n",
      "Average test accuracy: 0.698826\n",
      "Variance test accuracy: 0.000024\n",
      "Min test accuracy: 0.690740\n",
      "Max test accuracy: 0.705571\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "\n",
    "# Model parameters\n",
    "lamb = 0.001\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "max_acc = 0\n",
    "w2 = []\n",
    "print(\"Accuracy for a jet = 1\")\n",
    "k_indices = build_k_indices(y2, k_fold, seed)\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test, weights = cross_validation(y2, data2, k_indices, k, ridge_regression, lambda_=lamb)\n",
    "    if (acc_test>max_acc):\n",
    "        w2 = weights\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for a jet = 2 or 3\n",
      "0 - Training accuracy: 0.712113 / Test accuracy : 0.714089\n",
      "1 - Training accuracy: 0.712542 / Test accuracy : 0.707610\n",
      "2 - Training accuracy: 0.712235 / Test accuracy : 0.709815\n",
      "3 - Training accuracy: 0.712174 / Test accuracy : 0.710505\n",
      "4 - Training accuracy: 0.711209 / Test accuracy : 0.717811\n",
      "5 - Training accuracy: 0.712435 / Test accuracy : 0.709402\n",
      "6 - Training accuracy: 0.711868 / Test accuracy : 0.712297\n",
      "7 - Training accuracy: 0.711715 / Test accuracy : 0.714502\n",
      "8 - Training accuracy: 0.712021 / Test accuracy : 0.715054\n",
      "9 - Training accuracy: 0.712680 / Test accuracy : 0.707747\n",
      "\n",
      "Average test accuracy: 0.711883\n",
      "Variance test accuracy: 0.000010\n",
      "Min test accuracy: 0.707610\n",
      "Max test accuracy: 0.717811\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "\n",
    "# Model parameters\n",
    "lamb = 0.001\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "max_acc = 0\n",
    "w3 = []\n",
    "\n",
    "print(\"Accuracy for a jet = 2 or 3\")\n",
    "k_indices = build_k_indices(y3, k_fold, seed)\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test, weights = cross_validation(y3, data3, k_indices, k, ridge_regression, lambda_=lamb)\n",
    "    if (acc_test>max_acc):\n",
    "        w3 = weights\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(962,)\n"
     ]
    }
   ],
   "source": [
    "print(w1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cross_validation() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-67833cf8538f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0macc_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0maccs_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0maccs_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cross_validation() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y1, k_fold, seed)\n",
    "\n",
    "# Model parameters\n",
    "gamma = 0.01\n",
    "max_iter = 500\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation(y1, data1, k_indices, k, logistic_regression, initial_w, max_iters=max_iter, gamma=gamma)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_gradient_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-99f5ed721208>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Thomas\\workspace\\ML\\Machinus-Learningus\\Thomas\\implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mjust\u001b[0m \u001b[0muse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mas\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mto\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpenalized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Thomas\\workspace\\ML\\Machinus-Learningus\\Thomas\\implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;31m# converge criterion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevious_loss\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Thomas\\workspace\\ML\\Machinus-Learningus\\Thomas\\implementations.py\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[1;34m(y, tx, w, lambda_, gamma)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'calculate_gradient_log' is not defined"
     ]
    }
   ],
   "source": [
    "weights, loss = logistic_regression(y1, data1, initial_w, 500, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Training accuracy: 0.744948 / Test accuracy : 0.743970\n",
      "1 - Training accuracy: 0.768925 / Test accuracy : 0.768091\n",
      "2 - Training accuracy: 0.763354 / Test accuracy : 0.773696\n",
      "3 - Training accuracy: 0.789155 / Test accuracy : 0.784406\n",
      "4 - Training accuracy: 0.745048 / Test accuracy : 0.743069\n",
      "5 - Training accuracy: 0.527586 / Test accuracy : 0.526374\n",
      "6 - Training accuracy: 0.807960 / Test accuracy : 0.803123\n",
      "7 - Training accuracy: 0.642801 / Test accuracy : 0.641077\n",
      "8 - Training accuracy: 0.665899 / Test accuracy : 0.655790\n",
      "9 - Training accuracy: 0.745193 / Test accuracy : 0.741768\n",
      "\n",
      "Average test accuracy: 0.718136\n",
      "Variance test accuracy: 0.006564\n",
      "Min test accuracy: 0.526374\n",
      "Max test accuracy: 0.803123\n"
     ]
    }
   ],
   "source": [
    "k_fold = 10\n",
    "\n",
    "# Split data in k-fold\n",
    "k_indices = build_k_indices(y1, k_fold, seed)\n",
    "\n",
    "# Model parameters\n",
    "gamma = 0.01\n",
    "max_iter = 500\n",
    "lambda_ = 0.1\n",
    "\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    acc_train, acc_test = cross_validation(y1, data1, k_indices, k, reg_logistic_regression, lambda_=lambda_, initial_w = None, max_iters=max_iter, gamma=gamma)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "    \n",
    "for i in range(len(accs_train)):\n",
    "    print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (i, accs_train[i], accs_test[i]))\n",
    "\n",
    "print(\"\\nAverage test accuracy: %f\" % np.mean(accs_test))\n",
    "print(\"Variance test accuracy: %f\" % np.var(accs_test))\n",
    "print(\"Min test accuracy: %f\" % np.min(accs_test))\n",
    "print(\"Max test accuracy: %f\" % np.max(accs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from crossvalidation import *\n",
    "from implementations import *\n",
    "\n",
    "data1, data2, data3, y1, y2, y3 = split_models_and_process(tX, y)\n",
    "\n",
    "data3_aug = data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge3_test():\n",
    "    k_fold = 4\n",
    "\n",
    "    # Model parameters\n",
    "    lamb = 0.001\n",
    "\n",
    "    accs_train = []\n",
    "    accs_test = []\n",
    "\n",
    "    k_indices = build_k_indices(y3, k_fold, seed)\n",
    "\n",
    "    for k in range(k_fold):\n",
    "        acc_train, acc_test = cross_validation(y3, data3_aug, k_indices, k, ridge_regression, lambda_=lamb)\n",
    "        accs_train.append(acc_train)\n",
    "        accs_test.append(acc_test)\n",
    "\n",
    "    return np.mean(accs_test), np.max(accs_test)\n",
    "\n",
    "def not_tested(i, j):\n",
    "    test = [(3, 11), (2, 6), (6, 29), (2, 2), (2, 31), (3, 6), (2, 29), (18, 29)]\n",
    "    return not((i, j) in test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-cd96a38d641d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnot_tested\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mdata3_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdata3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mridge3_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_mean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mmax_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-113-8b90144522d4>\u001b[0m in \u001b[0;36mridge3_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0macc_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata3_aug\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlamb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0maccs_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0maccs_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Thomas\\workspace\\PROJET_ML\\crossvalidation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, regression_method, **args)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i_max, i_mean, j_max, j_mean = 0, 0, 0, 0\n",
    "max_mean, max_maxi = 0, 0\n",
    "acc_mean, acc_max = 0, 0\n",
    "for i in range(len(data3[0])):\n",
    "    for j in range(len(data3[0])):\n",
    "        if (not_tested(i, j)):\n",
    "            data3_aug = np.append(data3, np.array([(data3[:, i] * data3[:, j])]).T, axis=1)\n",
    "            mean, maxi = ridge3_test()\n",
    "            if mean > max_mean:\n",
    "                max_mean = mean\n",
    "                i_mean = i\n",
    "                j_mean = j\n",
    "            if maxi > max_maxi:\n",
    "                max_maxi = maxi\n",
    "                i_max = i\n",
    "                j_max = j\n",
    "            \n",
    "print(\"i_max: %d, j_max: %d, i_mean: %d, j_mean: %d\" % (i_max, j_max, i_mean, j_mean))\n",
    "print(\"new_mean: %f, new_max: %f\" % (max_mean, max_maxi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation results:\n",
    "\n",
    "columns 3 and 11 are the most correlated\n",
    "Then 2 and 6\n",
    "then 6 and 29\n",
    "then 2 and 2\n",
    "then (3 and 6) for the mean, and max accuracy increases most with (2, 31)\n",
    "then (18 and 29) for the mean, and max accuracy increases most with (2, 29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Processing the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred1, y_pred2, y_pred3 = predict_labels(w1, test1), predict_labels(w2, test2), predict_labels(w3, test3)\n",
    "y_pred = np.concatenate([y_pred1, y_pred2, y_pred3])\n",
    "ids_pred = np.concatenate([id1, id2, id3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
